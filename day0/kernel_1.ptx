//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-35583870
// Cuda compilation tools, release 12.8, V12.8.93
// Based on NVVM 7.0.1
//

// Version of the PTX ISA (Instruction Set Architecture)
.version 8.7
// Target architecture (SM 86 = compute capability 8.6, like Ampere architecture)
.target sm_86
// Address size (64-bit)
.address_size 64

// .globl means the function is globally visible (can be called from other modules)
	// .globl	_Z11sgemm_naiveiiifPfPKffS_

// Function declaration - this is a kernel that can be launched from CPU code
// _Z11sgemm_naive is the mangled name for the function "sgemm_naive"
// Function signature:
//   sgemm_naive(int M, int N, int K, float alpha, float* C, const float* A, float beta, float* B)
// This implements C = alpha * (A * B) + beta * C
.visible .entry _Z11sgemm_naiveiiifPfPKffS_(
	// Parameters declaration
	.param .u32 _Z11sgemm_naiveiiifPfPKffS__param_0,    // M (rows in C and A)
	.param .u32 _Z11sgemm_naiveiiifPfPKffS__param_1,    // N (columns in C and B)
	.param .u32 _Z11sgemm_naiveiiifPfPKffS__param_2,    // K (columns in A, rows in B)
	.param .f32 _Z11sgemm_naiveiiifPfPKffS__param_3,    // alpha (scalar multiplier)
	.param .u64 _Z11sgemm_naiveiiifPfPKffS__param_4,    // C matrix pointer
	.param .u64 _Z11sgemm_naiveiiifPfPKffS__param_5,    // A matrix pointer
	.param .f32 _Z11sgemm_naiveiiifPfPKffS__param_6,    // beta (scalar multiplier)
	.param .u64 _Z11sgemm_naiveiiifPfPKffS__param_7     // B matrix pointer
)
{
	// Register declarations
	.reg .pred 	%p<9>;          // Predicate registers (boolean values)
	.reg .f32 	%f<35>;         // 32-bit floating point registers
	.reg .b32 	%r<62>;         // 32-bit general purpose registers
	.reg .b64 	%rd<29>;        // 64-bit general purpose registers (addresses)

	// Load parameters into registers
	ld.param.u32 	%r36, [_Z11sgemm_naiveiiifPfPKffS__param_0];    // Load M
	ld.param.u32 	%r34, [_Z11sgemm_naiveiiifPfPKffS__param_1];    // Load N
	ld.param.u32 	%r35, [_Z11sgemm_naiveiiifPfPKffS__param_2];    // Load K
	ld.param.f32 	%f8, [_Z11sgemm_naiveiiifPfPKffS__param_3];     // Load alpha
	ld.param.u64 	%rd4, [_Z11sgemm_naiveiiifPfPKffS__param_4];    // Load C pointer
	ld.param.u64 	%rd5, [_Z11sgemm_naiveiiifPfPKffS__param_5];    // Load A pointer
	ld.param.f32 	%f9, [_Z11sgemm_naiveiiifPfPKffS__param_6];     // Load beta
	ld.param.u64 	%rd3, [_Z11sgemm_naiveiiifPfPKffS__param_7];    // Load B pointer
	
	// Convert host pointers to device global memory addresses
	cvta.to.global.u64 	%rd1, %rd5;    // Convert A pointer to global memory address
	cvta.to.global.u64 	%rd2, %rd4;    // Convert C pointer to global memory address
	
	// Calculate thread indices
	// Each thread will compute one element of the output matrix C
	mov.u32 	%r37, %ntid.x;        // Get block dimensions X (threads per block in X)
	mov.u32 	%r38, %ctaid.x;       // Get block index X
	mov.u32 	%r39, %tid.x;         // Get thread index X within block
	mad.lo.s32 	%r1, %r38, %r37, %r39; // Calculate global thread ID X: blockIdx.x * blockDim.x + threadIdx.x
	                                     // This is the row index in C
	
	mov.u32 	%r40, %ntid.y;        // Get block dimensions Y
	mov.u32 	%r41, %ctaid.y;       // Get block index Y
	mul.lo.s32 	%r2, %r41, %r40;      // Calculate blockIdx.y * blockDim.y
	mov.u32 	%r3, %tid.y;          // Get thread index Y within block
	add.s32 	%r4, %r2, %r3;        // Calculate global thread ID Y: blockIdx.y * blockDim.y + threadIdx.y
	                                 // This is the column index in C
	
	// Check if this thread is within matrix bounds
	setp.ge.u32 	%p1, %r1, %r36;    // Check if row index (%r1) >= M (%r36)
	setp.ge.u32 	%p2, %r4, %r34;    // Check if column index (%r4) >= N (%r34)
	or.pred  	%p3, %p1, %p2;         // Combine predicates with OR
	@%p3 bra 	$L__BB0_9;             // If out of bounds, branch to exit

	// Check if K dimension is valid
	setp.lt.s32 	%p4, %r35, 1;      // Check if K < 1
	mov.f32 	%f34, 0f00000000;       // Initialize accumulator to 0.0f
	@%p4 bra 	$L__BB0_8;             // If K < 1, skip computation and go to scaling/storing

	// Set up for the main computation loop
	mul.lo.s32 	%r5, %r1, %r35;       // Calculate row_offset = row_index * K
	and.b32  	%r61, %r35, 3;        // Calculate K % 4 (remainder for loop unrolling)
	add.s32 	%r43, %r35, -1;        // K - 1
	setp.lt.u32 	%p5, %r43, 3;      // Check if K-1 < 3 (if K < 4, too small for unrolling)
	mov.f32 	%f34, 0f00000000;       // Initialize accumulator to 0.0f
	mov.u32 	%r58, 0;               // Initialize loop counter to 0
	@%p5 bra 	$L__BB0_5;             // If K is too small for unrolling, skip to remainder handling

	// Setup for the main computation loop (unrolled by 4)
	add.s32 	%r56, %r5, 3;          // row_offset + 3 (for the 4th element)
	add.s32 	%r45, %r3, %r34;       // threadIdx.y + N
	add.s32 	%r55, %r45, %r2;       // threadIdx.y + N + (blockIdx.y * blockDim.y)
	shl.b32 	%r9, %r34, 2;          // N * 4 (stride for moving 4 rows in B)
	shl.b32 	%r46, %r34, 1;         // N * 2 (stride for moving 2 rows in B)
	add.s32 	%r54, %r4, %r46;       // column_index + N*2 (for 3rd element)
	mad.lo.s32 	%r53, %r34, 3, %r4;   // column_index + N*3 (for 4th element)
	sub.s32 	%r12, %r61, %r35;      // remainder - K (will be negative)
	mov.f32 	%f34, 0f00000000;       // Initialize accumulator to 0.0f
	mov.u32 	%r58, 0;               // Initialize loop counter to 0
	mov.u32 	%r52, %r4;             // Initialize column_index for 1st element

	// Main computation loop - processes 4 elements at a time
$L__BB0_4:
	// Process 1st element
	add.s32 	%r47, %r56, -3;         // Calculate A index for 1st element
	mul.wide.u32 	%rd6, %r47, 4;      // Calculate byte offset for A (4 bytes per float)
	add.s64 	%rd7, %rd2, %rd6;       // Get pointer to A element
	mul.wide.u32 	%rd8, %r52, 4;      // Calculate byte offset for B
	add.s64 	%rd9, %rd1, %rd8;       // Get pointer to B element
	ld.global.f32 	%f14, [%rd9];       // Load B element
	ld.global.f32 	%f15, [%rd7];       // Load A element
	fma.rn.f32 	%f16, %f15, %f14, %f34; // Accumulate: result += A * B
	
	// Process 2nd element
	add.s32 	%r48, %r56, -2;         // Calculate A index for 2nd element
	mul.wide.u32 	%rd10, %r48, 4;     // Calculate byte offset for A
	add.s64 	%rd11, %rd2, %rd10;     // Get pointer to A element
	mul.wide.u32 	%rd12, %r55, 4;     // Calculate byte offset for B
	add.s64 	%rd13, %rd1, %rd12;     // Get pointer to B element
	ld.global.f32 	%f17, [%rd13];      // Load B element
	ld.global.f32 	%f18, [%rd11];      // Load A element
	fma.rn.f32 	%f19, %f18, %f17, %f16; // Accumulate: result += A * B
	
	// Process 3rd element
	add.s32 	%r49, %r56, -1;         // Calculate A index for 3rd element
	mul.wide.u32 	%rd14, %r49, 4;     // Calculate byte offset for A
	add.s64 	%rd15, %rd2, %rd14;     // Get pointer to A element
	mul.wide.u32 	%rd16, %r54, 4;     // Calculate byte offset for B
	add.s64 	%rd17, %rd1, %rd16;     // Get pointer to B element
	ld.global.f32 	%f20, [%rd17];      // Load B element
	ld.global.f32 	%f21, [%rd15];      // Load A element
	fma.rn.f32 	%f22, %f21, %f20, %f19; // Accumulate: result += A * B
	
	// Process 4th element
	mul.wide.u32 	%rd18, %r56, 4;     // Calculate byte offset for A
	add.s64 	%rd19, %rd2, %rd18;     // Get pointer to A element
	mul.wide.u32 	%rd20, %r53, 4;     // Calculate byte offset for B
	add.s64 	%rd21, %rd1, %rd20;     // Get pointer to B element
	ld.global.f32 	%f23, [%rd21];      // Load B element
	ld.global.f32 	%f24, [%rd19];      // Load A element
	fma.rn.f32 	%f34, %f24, %f23, %f22; // Accumulate: result += A * B
	
	// Update indices for next iteration
	add.s32 	%r56, %r56, 4;          // Increment A index by 4 (next 4 elements)
	add.s32 	%r55, %r55, %r9;        // Increment B indices (move down 4 rows)
	add.s32 	%r54, %r54, %r9;
	add.s32 	%r53, %r53, %r9;
	add.s32 	%r52, %r52, %r9;
	
	// Loop control
	add.s32 	%r58, %r58, 4;          // Increment loop counter by 4
	add.s32 	%r50, %r12, %r58;       // Check if we've processed all elements
	setp.ne.s32 	%p6, %r50, 0;       // Set predicate if not done
	@%p6 bra 	$L__BB0_4;              // Loop if more elements to process

	// Handle remainder elements (K % 4)
$L__BB0_5:
	setp.eq.s32 	%p7, %r61, 0;       // Check if remainder is 0
	@%p7 bra 	$L__BB0_8;              // If no remainder, skip to next section

	// Setup for remainder loop
	mad.lo.s32 	%r60, %r58, %r34, %r4; // Calculate B index for remainder
	add.s32 	%r59, %r58, %r5;        // Calculate A index for remainder

	// Process remaining elements one by one
$L__BB0_7:
	.pragma "nounroll";                 // Compiler directive: don't unroll this loop
	mul.wide.u32 	%rd22, %r59, 4;     // Calculate byte offset for A
	add.s64 	%rd23, %rd2, %rd22;      // Get pointer to A element
	mul.wide.u32 	%rd24, %r60, 4;     // Calculate byte offset for B
	add.s64 	%rd25, %rd1, %rd24;      // Get pointer to B element
	ld.global.f32 	%f25, [%rd25];      // Load B element
	ld.global.f32 	%f26, [%rd23];      // Load A element
	fma.rn.f32 	%f34, %f26, %f25, %f34; // Accumulate: result += A * B
	
	// Update indices for next iteration
	add.s32 	%r60, %r60, %r34;       // Increment B index by N (move to next row)
	add.s32 	%r59, %r59, 1;          // Increment A index by 1 (move to next column)
	add.s32 	%r61, %r61, -1;         // Decrement remainder counter
	setp.ne.s32 	%p8, %r61, 0;       // Check if more elements to process
	@%p8 bra 	$L__BB0_7;              // Loop if more elements remain

	// Final scaling and storage
$L__BB0_8:
	// Calculate index in output matrix C
	mad.lo.s32 	%r51, %r1, %r34, %r4;  // index = row_index * N + column_index
	cvta.to.global.u64 	%rd26, %rd3;    // Convert output matrix pointer
	mul.wide.u32 	%rd27, %r51, 4;     // Calculate byte offset (4 bytes per float)
	add.s64 	%rd28, %rd26, %rd27;     // Get pointer to output element
	ld.global.f32 	%f27, [%rd28];      // Load current C value
	mul.f32 	%f28, %f27, %f9;         // Scale by beta: C_value * beta
	fma.rn.f32 	%f29, %f34, %f8, %f28;  // Final value: result * alpha + C_value * beta
	st.global.f32 	[%rd28], %f29;      // Store result back to C

	// Function exit
$L__BB0_9:
	ret;                                // Return from function
}